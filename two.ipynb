{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        # Forward pass\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = np.tanh(self.Z1)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def backward_propagation(self, X, Y,learn):\n",
    "        m = X.shape[1]  # Number of samples\n",
    "\n",
    "        # Backward pass\n",
    "        dZ2 = self.A2 - Y\n",
    "        dW2 = (1 / m) * np.dot(dZ2, self.A1.T)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * (1 - np.power(self.A1, 2))  # derivative of tanh\n",
    "        dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        # Update parameters\n",
    "        self.W2 -= dW2*learn\n",
    "        self.b2 -= db2*learn\n",
    "        self.W1 -= dW1*learn\n",
    "        self.b1 -= db1*learn\n",
    "\n",
    "    def train(self, X, Y, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Forward propagation\n",
    "            predictions = self.forward_propagation(X)\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            loss = self.cross_entropy_loss(Y, predictions)\n",
    "\n",
    "            # Backward propagation\n",
    "            self.backward_propagation(X, Y,learning_rate)\n",
    "\n",
    "            # Print the loss every 100 epochs\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def cross_entropy_loss(self, Y, A):\n",
    "        m = Y.shape[1]  # Number of samples\n",
    "        return -(1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "\n",
    "# Load and preprocess the data\n",
    "X, Y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X = X.T\n",
    "Y = Y.reshape(1, -1)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.ravel(), cmap=plt.cm.Spectral)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Planar Data Classification Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Example usage\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "model.train(X, Y, num_epochs=10000, learning_rate=0.01)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
